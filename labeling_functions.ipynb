{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# snorkel\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnid_pth = \"./data/FNID/fake news detection(FakeNewsNet)/fnn_train.csv\"\n",
    "liar_pth = \"./data/FNID/fake news detection(LIAR)/liar_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "FAKE = 0\n",
    "REAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>sources</th>\n",
       "      <th>paragraph_based_content</th>\n",
       "      <th>fullText_based_content</th>\n",
       "      <th>label_fnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3106</td>\n",
       "      <td>2011-01-25T06:00:00-05:00</td>\n",
       "      <td>Joe Wilkinson</td>\n",
       "      <td>A national organization says Georgia has one o...</td>\n",
       "      <td>['http://www.ajc.com/news/georgia-politics-ele...</td>\n",
       "      <td>['A coalition of government watchdog groups la...</td>\n",
       "      <td>A coalition of government watchdog groups last...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5655</td>\n",
       "      <td>2012-04-02T11:42:20-04:00</td>\n",
       "      <td>Rick Scott</td>\n",
       "      <td>Says Barack Obama's health care law \"will be t...</td>\n",
       "      <td>['http://www.youtube.com/watch?v=TaC0mKApf9Q&amp;f...</td>\n",
       "      <td>['As Supreme Court justices embarked on three ...</td>\n",
       "      <td>As Supreme Court justices embarked on three da...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                       date        speaker  \\\n",
       "0  3106  2011-01-25T06:00:00-05:00  Joe Wilkinson   \n",
       "1  5655  2012-04-02T11:42:20-04:00     Rick Scott   \n",
       "\n",
       "                                           statement  \\\n",
       "0  A national organization says Georgia has one o...   \n",
       "1  Says Barack Obama's health care law \"will be t...   \n",
       "\n",
       "                                             sources  \\\n",
       "0  ['http://www.ajc.com/news/georgia-politics-ele...   \n",
       "1  ['http://www.youtube.com/watch?v=TaC0mKApf9Q&f...   \n",
       "\n",
       "                             paragraph_based_content  \\\n",
       "0  ['A coalition of government watchdog groups la...   \n",
       "1  ['As Supreme Court justices embarked on three ...   \n",
       "\n",
       "                              fullText_based_content label_fnn  \n",
       "0  A coalition of government watchdog groups last...      fake  \n",
       "1  As Supreme Court justices embarked on three da...      fake  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(fnid_pth)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15212 entries, 0 to 15211\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   id                       15212 non-null  int64 \n",
      " 1   date                     15212 non-null  object\n",
      " 2   speaker                  15212 non-null  object\n",
      " 3   statement                15212 non-null  object\n",
      " 4   sources                  15212 non-null  object\n",
      " 5   paragraph_based_content  15212 non-null  object\n",
      " 6   fullText_based_content   15212 non-null  object\n",
      " 7   label_fnn                15212 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 950.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the label to numbers, to use it for the validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label_numeric\"] = data.apply(lambda x: int(x[\"label_fnn\"] == \"real\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the sentiment analysis package, to use later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriving the labels or valuable information from each site\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # contacts a url, downloads the website's content and parses it.\n",
    "# def get_parsed_html(url):\n",
    "#     req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#     webpage = urlopen(req).read()\n",
    "#     parsed_html = BeautifulSoup(webpage)\n",
    "#     return parsed_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.politifact.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_poitifact_image_alt(url):\n",
    "#     result = \"abstain\"\n",
    "#     try:\n",
    "#         parsed_html = get_parsed_html(url)\n",
    "#         div = parsed_html.body.find(\"div\", attrs={\"class\": \"m-statement__meter\"})\n",
    "#         result = div.find(\"img\", attrs={\"class\": \"c-image__original\"})[\"alt\"]\n",
    "#         time.sleep(3)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.snopes.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_snopes_image_alt(url):\n",
    "#     result = \"abstain\"\n",
    "#     try:\n",
    "#         parsed_html = get_parsed_html(url)\n",
    "#         div = parsed_html.body.find(\"div\", attrs={\"class\": \"media rating\"})\n",
    "#         result = div.find(\"img\")[\"alt\"]\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.factcheck.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_factcheck_first_paragraph(url):\n",
    "#     result = \"abstain\"\n",
    "#     try:\n",
    "#         parsed_html = get_parsed_html(url)\n",
    "#         div = parsed_html.body.find(\"div\", attrs={\"class\": \"entry-content\"})\n",
    "#         # if the first paragraph starts with 'Q:' and the second with 'A:' than it is a Q & A style;\n",
    "#         # take the second paragraph\n",
    "#         # otherwise take the first.\n",
    "#         parag = div.find_all(\"p\")\n",
    "#         if parag[0].text[0:3] == \"Q: \" and parag[1].text[0:3] == \"A: \":\n",
    "#             return parag[1].text\n",
    "#         return parag[0].text\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.factcheck.afp.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_factcheck_afp_title(url):\n",
    "#     result = \"abstain\"\n",
    "#     try:\n",
    "#         parsed_html = get_parsed_html(url)\n",
    "#         h3 = parsed_html.body.find(\"h3\")\n",
    "#         return h3.text\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.twitter.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_twitter_name(url):\n",
    "#     start = url.find(\"https\")\n",
    "#     sub = url[20 + start : len(url)]  # removing 'https://twitter.com/'\n",
    "#     index = sub.find(\"/\")\n",
    "#     if index == -1:\n",
    "#         return sub\n",
    "#     else:\n",
    "#         return sub[:index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving urls of fact checking sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checking_sites = {\n",
    "    \"www.politifact.com\": None,  # get_poitifact_image_alt\n",
    "    \"www.snopes.com\": None,  # get_snopes_image_alt\n",
    "    \"www.twitter.com\": None,  # extract_twitter_name\n",
    "    \"www.factcheck.org\": None,  # get_factcheck_first_paragraph\n",
    "    \"factcheck.afp.com\": None,  # get_factcheck_afp_title\n",
    "    \"www.washingtonpost.com/news/fact-checker\": None,\n",
    "    \"www.realclearpolitics.com\": None,\n",
    "    \"www.glennbeck.com\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sources_as_list(source, domain):\n",
    "#     urls = source[1:-1].split(\",\")\n",
    "#     u = []\n",
    "#     for url in urls:\n",
    "#         if domain in url:\n",
    "#             u.append(url)\n",
    "#     return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15212"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the new columns\n",
    "for site in fact_checking_sites:\n",
    "    data[site] = None\n",
    "data_size = data.shape[0]\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checking_sites_results = {\n",
    "    \"www.politifact.com\": [None] * data_size,\n",
    "    \"www.snopes.com\": [None] * data_size,\n",
    "    \"www.twitter.com\": [None] * data_size,\n",
    "    \"www.factcheck.org\": [None] * data_size,\n",
    "    \"factcheck.afp.com\": [None] * data_size,\n",
    "    \"www.washingtonpost.com/news/fact-checker\": [None] * data_size,\n",
    "    \"www.realclearpolitics.com\": [None] * data_size,\n",
    "    \"www.glennbeck.com\": [None] * data_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the records\n",
    "# and looks through the sources for each fact-checking site\n",
    "#\n",
    "# Commented out because it takes hours to run (the sites will throttle too many requests)\n",
    "# the results are presented below.\n",
    "\n",
    "\n",
    "# with open(\"factchecking_results.txt\", \"a\") as results:\n",
    "#     for i, row in data.iterrows():\n",
    "#         for site in fact_checking_sites:\n",
    "#             sources = sources_as_list(row[\"sources\"], site)\n",
    "#             if len(sources) != 0:\n",
    "#                 # print(\"{}\".format(i))\n",
    "#                 labels = \"\"\n",
    "#                 for source in sources:\n",
    "#                     handler = fact_checking_sites[site]\n",
    "#                     if handler:\n",
    "#                         # print(\"Handling: {} ++++++++++++++++++++++++++\".format(site))\n",
    "#                         source = str(source).strip()[1:-1]\n",
    "#                         if len(labels) > 0:\n",
    "#                             labels += \", \" + handler(str(source))\n",
    "#                         else:\n",
    "#                             labels += handler(str(source))\n",
    "#                         # print(\"Result: {} ++++++++++++++++++++++++++\".format(labels))\n",
    "#                     else:\n",
    "#                         if len(labels) > 0:\n",
    "#                             # print(\"Handling: {} ++++++++++++++++++++++++++\".format(site))\n",
    "#                             labels += \", \" + source\n",
    "#                         else:\n",
    "#                             labels += source\n",
    "#                     # print(\"Result: {} ++++++++++++++++++++++++++\".format(labels))\n",
    "#                 fact_checking_sites_results[site][i] = labels\n",
    "#                 print(\"{} | {} | {}\".format(i, site, labels))\n",
    "#                 results.write(\"{} | {} | {}\\n\".format(i, site, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for site in fact_checking_sites:\n",
    "#     data[site] = fact_checking_sites_results[site]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALTERNATIVE TO THE TWO COMMENTED OUT CELLS ABOVE. THE RESULTS OF CALLING THE APIS HAVE BEEN SERIALIZED TO FILE\n",
    "### FOR REPRODUCABILITY, AND TO SAVE TIME.\n",
    "apiResultsFile = open(\"./data/apiResults.txt\", \"r\", encoding=\"utf-8\")\n",
    "for line in apiResultsFile:\n",
    "    try:\n",
    "        sr = line.split(\"|\")\n",
    "        row = int(sr[0].strip())\n",
    "        col = sr[1].strip()\n",
    "        data.at[row, col] = sr[2]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "apiResultsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowdsourcing - reading the results from the rated files, and adding them to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.glennbeck.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glenbeck_ratings = pd.read_csv(\"./data/glennbeck_ratings.csv\")\n",
    "\n",
    "for i, row in glenbeck_ratings.iterrows():\n",
    "    data.loc[data[\"id\"] == row[\"id\"], [\"www.glennbeck.com\"]] = row[\"www.glennbeck.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.realclearpolitics.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ratings = pd.read_csv(\"./data/realclearpolitics_ratings.csv\")\n",
    "\n",
    "for i, row in rp_ratings.iterrows():\n",
    "    data.loc[data[\"id\"] == row[\"id\"], [\"www.realclearpolitics.com\"]] = row[\n",
    "        \"www.realclearpolitics.com\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### www.washingtonpost.com/news/fact-checker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_ratings = pd.read_csv(\"./data/washingtonpost_ratings.csv\")\n",
    "\n",
    "for i, row in wp_ratings.iterrows():\n",
    "    data.loc[\n",
    "        data[\"id\"] == row[\"id\"], [\"www.washingtonpost.com/news/fact-checker\"]\n",
    "    ] = row[\"www.washingtonpost.com/news/fact-checker\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the labels with Snorkel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def label_snopes(row):\n",
    "    label = row[\"www.snopes.com\"]\n",
    "    if label is not None:\n",
    "        label = str(row[\"www.snopes.com\"])\n",
    "        if \"real\" in label:\n",
    "            return REAL\n",
    "        else:\n",
    "            return FAKE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def label_wp(row):\n",
    "    label = row[\"www.washingtonpost.com/news/fact-checker\"]\n",
    "    if label is not None:\n",
    "        label = str(row[\"www.washingtonpost.com/news/fact-checker\"])\n",
    "        if \"real\" in label:\n",
    "            return REAL\n",
    "        else:\n",
    "            return FAKE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def label_rp(row):\n",
    "    label = row[\"www.realclearpolitics.com\"]\n",
    "    if label is not None:\n",
    "        label = str(row[\"www.realclearpolitics.com\"])\n",
    "        if \"real\" in label:\n",
    "            return REAL\n",
    "        else:\n",
    "            return FAKE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_o_meter = {\n",
    "    \"true\": 4,\n",
    "    \"mostly-true\": 3,\n",
    "    \"half-true\": 2,\n",
    "    \"barely-true\": 1,\n",
    "    \"mostly-false\": -1,\n",
    "    \"false\": -2,\n",
    "    \"pants-fire\": -3,\n",
    "}\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def label_politifact(row):\n",
    "    total_score = 0\n",
    "    labels = row[\"www.politifact.com\"]\n",
    "    # print(labels)\n",
    "    if labels:\n",
    "        labels = str(row[\"www.politifact.com\"]).split(\",\")\n",
    "        # The last label has the newline character\n",
    "        if len(labels) > 0:\n",
    "            labels[-1] = labels[-1][:-2]\n",
    "        for label in labels:\n",
    "            # print(label)\n",
    "            label = label.strip()\n",
    "            if label in truth_o_meter:\n",
    "                total_score += truth_o_meter[label]\n",
    "    # print(\"score: {} \".format(total_score))\n",
    "    if total_score > 0:\n",
    "        return REAL\n",
    "    if total_score < 0:\n",
    "        return FAKE\n",
    "\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factcheck_sentiment(row, columnName):\n",
    "    label = str(row[columnName])\n",
    "    score = 0\n",
    "    if label:\n",
    "        claims = label[1:-1].split(\",\")\n",
    "        for claim in claims:\n",
    "            # print(claim)\n",
    "            sentiment = sid.polarity_scores(claim)\n",
    "            # print(sentiment)\n",
    "            if sentiment[\"neg\"] > sentiment[\"pos\"]:\n",
    "                score -= 1\n",
    "            elif sentiment[\"pos\"] > sentiment[\"neg\"]:\n",
    "                score += 1\n",
    "        if score > 0:\n",
    "            return REAL\n",
    "        elif score < 0:\n",
    "            return FAKE\n",
    "        else:\n",
    "            return ABSTAIN\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def factcheckqa_sentiment(row):\n",
    "    return factcheck_sentiment(row, \"www.factcheck.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def factcheckafpqa_sentiment(row):\n",
    "    return factcheck_sentiment(row, \"factcheck.afp.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning from the liar dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>sources</th>\n",
       "      <th>paragraph_based_content</th>\n",
       "      <th>fullText_based_content</th>\n",
       "      <th>label-liar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18178</td>\n",
       "      <td>2020-03-18T13:26:42-04:00</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>\"COVID-19 started because we eat animals.\"</td>\n",
       "      <td>['https://www.cdc.gov/coronavirus/2019-ncov/ca...</td>\n",
       "      <td>['Vegan Instagram users are pinning the 2019 c...</td>\n",
       "      <td>Vegan Instagram users are pinning the 2019 cor...</td>\n",
       "      <td>barely-true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3350</td>\n",
       "      <td>2011-03-04T09:12:59-05:00</td>\n",
       "      <td>Glenn Beck</td>\n",
       "      <td>Says Michelle Obama has 43 people on her staff...</td>\n",
       "      <td>['http://www.glennbeck.com/2011/02/25/while-wo...</td>\n",
       "      <td>['Glenn Beck rekindled a falsehood about the s...</td>\n",
       "      <td>Glenn Beck rekindled a falsehood about the siz...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                       date          speaker  \\\n",
       "0  18178  2020-03-18T13:26:42-04:00  Instagram posts   \n",
       "1   3350  2011-03-04T09:12:59-05:00       Glenn Beck   \n",
       "\n",
       "                                           statement  \\\n",
       "0         \"COVID-19 started because we eat animals.\"   \n",
       "1  Says Michelle Obama has 43 people on her staff...   \n",
       "\n",
       "                                             sources  \\\n",
       "0  ['https://www.cdc.gov/coronavirus/2019-ncov/ca...   \n",
       "1  ['http://www.glennbeck.com/2011/02/25/while-wo...   \n",
       "\n",
       "                             paragraph_based_content  \\\n",
       "0  ['Vegan Instagram users are pinning the 2019 c...   \n",
       "1  ['Glenn Beck rekindled a falsehood about the s...   \n",
       "\n",
       "                              fullText_based_content   label-liar  \n",
       "0  Vegan Instagram users are pinning the 2019 cor...  barely-true  \n",
       "1  Glenn Beck rekindled a falsehood about the siz...   pants-fire  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Liar dataset\n",
    "liar = pd.read_csv(liar_pth)\n",
    "liar.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['barely-true', 'pants-fire', 'half-true', 'mostly-true', 'true',\n",
       "       'false'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the unique labels\n",
    "labels = liar[\"label-liar\"].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "# true speakers\n",
    "counts_true = collections.Counter(\n",
    "    liar[(liar[\"label-liar\"] == \"mostly-true\") | (liar[\"label-liar\"] == \"true\")][\n",
    "        \"speaker\"\n",
    "    ]\n",
    ")\n",
    "counts_true = dict(counts_true.most_common())\n",
    "# false speakers\n",
    "counts_false = collections.Counter(\n",
    "    liar[(liar[\"label-liar\"] == \"false\") | (liar[\"label-liar\"] == \"pants-fire\")][\n",
    "        \"speaker\"\n",
    "    ]\n",
    ")\n",
    "counts_false = dict(counts_false.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_percent = {}\n",
    "for k, v in counts_false.items():\n",
    "    total = v\n",
    "    if k in counts_true:\n",
    "        total += counts_true[k]\n",
    "    false_percent[k] = v / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_percent = {}\n",
    "for k, v in counts_true.items():\n",
    "    total = v\n",
    "    if k in counts_false:\n",
    "        total += counts_false[k]\n",
    "    true_percent[k] = v / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def speaker(row):\n",
    "    speaker = row[\"speaker\"]\n",
    "    if speaker in true_percent and true_percent[speaker] > 0.6:\n",
    "        return REAL\n",
    "    if speaker in false_percent and false_percent[speaker] > 0.6:\n",
    "        return FAKE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the snorkel model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12170/12170 [00:01<00:00, 10716.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label_rp</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0.002219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_wp</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>0.003122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_snopes</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>0.004108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_politifact</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.244618</td>\n",
       "      <td>0.184717</td>\n",
       "      <td>0.071159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factcheckqa_sentiment</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>0.020049</td>\n",
       "      <td>0.010764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factcheckafpqa_sentiment</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.721282</td>\n",
       "      <td>0.216352</td>\n",
       "      <td>0.075842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts\n",
       "label_rp                  0   [0, 1]  0.007970  0.006984   0.002219\n",
       "label_wp                  1   [0, 1]  0.009860  0.008874   0.003122\n",
       "label_snopes              2      [0]  0.027691  0.026952   0.004108\n",
       "label_politifact          3   [0, 1]  0.244618  0.184717   0.071159\n",
       "factcheckqa_sentiment     4   [0, 1]  0.020789  0.020049   0.010764\n",
       "factcheckafpqa_sentiment  5   [0, 1]  0.000822  0.000822   0.000493\n",
       "speaker                   6   [0, 1]  0.721282  0.216352   0.075842"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1, random_state=1)\n",
    "df_train = data[:12170]\n",
    "df_valid = data[12170:]\n",
    "\n",
    "lfs = [\n",
    "    label_rp,\n",
    "    label_wp,\n",
    "    label_snopes,\n",
    "    label_politifact,\n",
    "    factcheckqa_sentiment,\n",
    "    factcheckafpqa_sentiment,\n",
    "    speaker,\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3042/3042 [00:00<00:00, 11302.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label_rp</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.007561</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_wp</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_snopes</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.028271</td>\n",
       "      <td>0.026956</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>0.895349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_politifact</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.241289</td>\n",
       "      <td>0.173899</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>452</td>\n",
       "      <td>282</td>\n",
       "      <td>0.615804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factcheckqa_sentiment</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>0.606557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factcheckafpqa_sentiment</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.704471</td>\n",
       "      <td>0.204142</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>1644</td>\n",
       "      <td>499</td>\n",
       "      <td>0.767149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "label_rp                  0   [0, 1]  0.007561  0.006903   0.003616       23   \n",
       "label_wp                  1   [0, 1]  0.012492  0.012163   0.005588       38   \n",
       "label_snopes              2      [0]  0.028271  0.026956   0.004274       77   \n",
       "label_politifact          3   [0, 1]  0.241289  0.173899   0.062459      452   \n",
       "factcheckqa_sentiment     4   [0, 1]  0.020053  0.018738   0.011177       37   \n",
       "factcheckafpqa_sentiment  5   [0, 1]  0.001644  0.001644   0.000986        2   \n",
       "speaker                   6   [0, 1]  0.704471  0.204142   0.067390     1644   \n",
       "\n",
       "                          Incorrect  Emp. Acc.  \n",
       "label_rp                          0   1.000000  \n",
       "label_wp                          0   1.000000  \n",
       "label_snopes                      9   0.895349  \n",
       "label_politifact                282   0.615804  \n",
       "factcheckqa_sentiment            24   0.606557  \n",
       "factcheckafpqa_sentiment          3   0.400000  \n",
       "speaker                         499   0.767149  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# majority_model = MajorityLabelVoter()\n",
    "# preds_train_majority = majority_model.predict(L=L_train)\n",
    "\n",
    "L_valid = applier.apply(df=df_valid)\n",
    "Y_valid = df_valid[\"label_numeric\"].values\n",
    "LFAnalysis(L_valid, lfs).lf_summary(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12170 entries, 3176 to 614\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                    Non-Null Count  Dtype \n",
      "---  ------                                    --------------  ----- \n",
      " 0   id                                        12170 non-null  int64 \n",
      " 1   date                                      12170 non-null  object\n",
      " 2   speaker                                   12170 non-null  object\n",
      " 3   statement                                 12170 non-null  object\n",
      " 4   sources                                   12170 non-null  object\n",
      " 5   paragraph_based_content                   12170 non-null  object\n",
      " 6   fullText_based_content                    12170 non-null  object\n",
      " 7   label_fnn                                 12170 non-null  object\n",
      " 8   label_numeric                             12170 non-null  int64 \n",
      " 9   www.politifact.com                        3746 non-null   object\n",
      " 10  www.snopes.com                            337 non-null    object\n",
      " 11  www.twitter.com                           2 non-null      object\n",
      " 12  www.factcheck.org                         373 non-null    object\n",
      " 13  factcheck.afp.com                         13 non-null     object\n",
      " 14  www.washingtonpost.com/news/fact-checker  120 non-null    object\n",
      " 15  www.realclearpolitics.com                 97 non-null     object\n",
      " 16  www.glennbeck.com                         18 non-null     object\n",
      "dtypes: int64(2), object(15)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[\"id\", \"statement\", \"label_fnn\", \"label_numeric\"]].to_csv(\n",
    "    \"./data/train_data.csv\"\n",
    ")\n",
    "df_valid[[\"id\", \"statement\", \"label_fnn\", \"label_numeric\"]].to_csv(\n",
    "    \"./data/valid_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/L_train.npy\", L_train)\n",
    "np.save(\"./data/L_valid.npy\", L_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
